<html>
<head>
<title>WikiLengths.java</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
WikiLengths.java</font>
</center></td></tr></table>
<pre><span class="s0">/* MapReduce Program - file WikiLengths.java 
 * Author: 
 * Thomas P. Moyer 
 */</span>

<span class="s2">package </span><span class="s1">drivers</span><span class="s2">;</span>

<span class="s2">import </span><span class="s1">java.io.IOException</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">java.text.SimpleDateFormat</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">java.util.Date</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.conf.Configuration</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.fs.FileSystem</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.fs.Path</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.io.LongWritable</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.io.NullWritable</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.mapreduce.Job</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.mapreduce.lib.input.FileInputFormat</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.hadoop.mapreduce.lib.output.FileOutputFormat</span><span class="s2">;</span>
<span class="s2">import </span><span class="s1">org.apache.log4j.Logger</span><span class="s2">;</span>

<span class="s3">/** MapReduce program for Length of Wikipedia articles.</span>
 <span class="s3">* This will output the length of a Wikipedia article.</span>
 <span class="s3">*/</span>

<span class="s1">@SuppressWarnings(</span><span class="s4">&quot;unused&quot;</span><span class="s1">)</span>
<span class="s2">public class </span><span class="s1">WikiLengths {</span>

    <span class="s0">/* this is intended as a beginners task... read the wikipedia dump file (43GB non zipped) 
     *  and output the feeder 
     * a histogram of number of pages vs number of bytes/page 
     * (each wikipedia article is one &lt;page&gt; &lt;/page&gt;) 
     */</span>

    <span class="s2">private static final transient </span><span class="s1">Logger logger = Logger.getLogger(</span><span class="s4">&quot;app&quot;</span><span class="s1">)</span><span class="s2">;</span>

    <span class="s2">public static void </span><span class="s1">runJob(String input</span><span class="s2">, </span><span class="s1">String output) </span><span class="s2">throws </span><span class="s1">IOException {</span>
        <span class="s1">logger.debug(</span><span class="s4">&quot;cme@ runJob with input=&quot; </span><span class="s1">+ input + </span><span class="s4">&quot;  output=&quot; </span><span class="s1">+ output)</span><span class="s2">;</span>
        <span class="s1">Configuration conf = </span><span class="s2">new </span><span class="s1">Configuration()</span><span class="s2">;</span>
        <span class="s1">conf.set(</span><span class="s4">&quot;xmlinput.start&quot;</span><span class="s2">, </span><span class="s4">&quot;&lt;page&gt;&quot;</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">conf.set(</span><span class="s4">&quot;xmlinput.end&quot;</span><span class="s2">, </span><span class="s4">&quot;&lt;/page&gt;&quot;</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">conf.set(</span>
                <span class="s4">&quot;io.serializations&quot;</span><span class="s2">,</span>
                <span class="s4">&quot;org.apache.hadoop.io.serializer.JavaSerialization,&quot;</span>
                        <span class="s1">+ </span><span class="s4">&quot;org.apache.hadoop.io.serializer.WritableSerialization&quot;</span>
        <span class="s1">)</span><span class="s2">;</span>

        <span class="s0">/* these two lines enable bzip output from the reducer 
         */</span>

        <span class="s0">//conf.setBoolean(&quot;mapred.output.compress&quot;, true);</span>
        <span class="s0">//conf.setClass  (&quot;mapred.output.compression.codec&quot;,</span>
        <span class="s0">//BZip2Codec.class,CompressionCodec.class);</span>

        <span class="s1">SimpleDateFormat ymdhms = </span><span class="s2">new </span><span class="s1">SimpleDateFormat(</span><span class="s4">&quot;yyyy-MM-dd'T'HH:mm:ss&quot;</span><span class="s1">)</span><span class="s2">;</span>

        <span class="s0">/* ISO 8601 format 
         */</span>

        <span class="s1">Job job = </span><span class="s2">new </span><span class="s1">Job(conf</span><span class="s2">, </span><span class="s4">&quot;wikPageLengths &quot; </span><span class="s1">+ ymdhms.format(</span><span class="s2">new </span><span class="s1">Date()))</span><span class="s2">;</span>

        <span class="s1">System.out.println(input)</span><span class="s2">;</span>
        <span class="s1">FileInputFormat.setInputPaths(job</span><span class="s2">, </span><span class="s1">input)</span><span class="s2">;</span>
        <span class="s1">job.setJarByClass(WikiLengths.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>

        <span class="s1">job.setMapperClass(mappers.WikiLengthsMapper.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>

        <span class="s0">/* job.setCombinerClass(WikiLengthsReducer.class); 
        This is how to get a mapper which never completes... 
        specify a reducer with different outputs from it's inputs as a combiner 
         */</span>

        <span class="s1">job.setReducerClass(reducers.WikiLengthsReducer.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s0">//job.setNumReduceTasks(0);</span>
        <span class="s1">job.setInputFormatClass(utils.XmlInputFormat.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">job.setMapOutputKeyClass(LongWritable.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s0">//job.setMapOutputValueClass(LongWritable.class);</span>

        <span class="s0">/* not necessary because reducer outputValueClass matches 
         */</span>

        <span class="s1">job.setOutputKeyClass(NullWritable.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">job.setOutputValueClass(LongWritable.</span><span class="s2">class</span><span class="s1">)</span><span class="s2">;</span>

        <span class="s1">Path outPath = </span><span class="s2">new </span><span class="s1">Path(output)</span><span class="s2">;</span>
        <span class="s1">FileOutputFormat.setOutputPath(job</span><span class="s2">, </span><span class="s1">outPath)</span><span class="s2">;</span>
        <span class="s1">FileSystem dfs = FileSystem.get(outPath.toUri()</span><span class="s2">, </span><span class="s1">conf)</span><span class="s2">;</span>
        <span class="s2">if </span><span class="s1">(dfs.exists(outPath)) {</span>
            <span class="s1">dfs.delete(outPath</span><span class="s2">, true</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">}</span>

        <span class="s2">try </span><span class="s1">{</span>
            <span class="s1">job.waitForCompletion(</span><span class="s2">true</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s1">} </span><span class="s2">catch </span><span class="s1">(InterruptedException ex) {</span>
            <span class="s0">//Logger.getLogger(WikiSee2.class.getName()).log(Level.SEVERE, null, ex);</span>
            <span class="s1">logger.fatal(</span><span class="s4">&quot;InterruptedException &quot; </span><span class="s1">+ WikiLengths.</span><span class="s2">class</span><span class="s1">.getName() + </span><span class="s4">&quot; &quot; </span><span class="s1">+ ex)</span><span class="s2">;</span>
        <span class="s1">} </span><span class="s2">catch </span><span class="s1">(ClassNotFoundException ex) {</span>
            <span class="s1">logger.fatal(</span><span class="s4">&quot;ClassNotFoundException &quot; </span><span class="s1">+ WikiLengths.</span><span class="s2">class</span><span class="s1">.getName() + </span><span class="s4">&quot; &quot; </span><span class="s1">+ ex)</span><span class="s2">;</span>
        <span class="s1">}</span>

    <span class="s1">}</span>

    <span class="s2">public static void </span><span class="s1">main(String[] args) {</span>
        <span class="s1">logger.debug(</span><span class="s4">&quot;cme@ main&quot;</span><span class="s1">)</span><span class="s2">;</span>
        <span class="s2">try </span><span class="s1">{</span>
            <span class="s1">runJob(args[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">args[</span><span class="s5">1</span><span class="s1">])</span><span class="s2">;</span>
        <span class="s1">} </span><span class="s2">catch </span><span class="s1">(IOException ex) {</span>
            <span class="s1">logger.fatal(</span><span class="s4">&quot;IOException &quot; </span><span class="s1">+ WikiLengths.</span><span class="s2">class</span><span class="s1">.getName() + </span><span class="s4">&quot; &quot; </span><span class="s1">+ ex)</span><span class="s2">;</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
</pre>
</body>
</html>